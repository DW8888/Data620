{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb3cb36",
   "metadata": {},
   "source": [
    "# Alfred Job Network Graph\n",
    "\n",
    "This notebook builds on the proposal in `FinalProjectProposa_Draft.pdf` and uses job data stored in the `alfred_db` PostgreSQL database.\n",
    "We will extract the latest postings generated by Alfred, engineer job & skill relationships, and analyze the resulting graph structure.\n",
    "LINK TO VIDEO HERE https://youtu.be/CCoCeZWMZCw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6843cd",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "There are some companian files in this project that help with runnign this notebook:\n",
    "'artifacts/Jobs_clean.csv' outputs cleaned job data from the database\n",
    "'artifacts/jobs_skills_map.csv' exploded job-skill mapping data\n",
    "'artifacts/jobs_clean.parquet' parquet version of cleaned job data\n",
    "'artifacts/jobs_skill_map.parquet' parquet version of job-skill mapping data\n",
    "\n",
    "I am pulling directly from the database in this notebook, but these files can be used to skip that step if needed.\n",
    "\n",
    "If your are pulling from the database, the workflow is as follows:\n",
    "\n",
    "1. Import necessary libraries for data manipulation, database connection, and graph analysis.\n",
    "2. Load environment variables (or manually provide credentials) that describe how to reach `alfred_db`.\n",
    "3. Connect to PostgreSQL, pull the `jobs` table into a pandas DataFrame, and perform light cleaning.\n",
    "4. Use spaCy-based NLP to infer skill phrases directly from each job description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q python-dotenv sqlalchemy psycopg2-binary pandas networkx matplotlib plotly-express tqdm spacy scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d269f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core analysis imports plus visualization + progress helpers used throughout the notebook.\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "from dotenv import load_dotenv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import kaleido\n",
    "import spacy\n",
    "from spacy.cli import download as spacy_download\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "pd.set_option(\"display.max_rows\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22756dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301b92f",
   "metadata": {},
   "source": [
    "## Load Environment Variables and Create a Database Engine\n",
    "\n",
    "I am Storing my database credentials in a `.env` file for security and convenience.\n",
    "But for your covenieence you can start at the section Titles:\"FROM CSV FILES\" below to skip the database connection step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e94f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "ALFRED_ROOT = (\n",
    "    PROJECT_ROOT.parents[0] / \"alfred\"\n",
    ").resolve()  # Look one level up for the Alfred repo.\n",
    "\n",
    "# Load whichever .env files exist so DATABASE_URL and friends are available without manual export.\n",
    "candidate_env_files = [PROJECT_ROOT / \".env\", ALFRED_ROOT / \".env\"]\n",
    "for env_path in candidate_env_files:\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path, override=False)\n",
    "        print(f\"Loaded environment variables from {env_path}\")\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "if not DATABASE_URL:\n",
    "    raise RuntimeError(\n",
    "        \"DATABASE_URL is not set. Create a .env file or export the variable before running this notebook.\"\n",
    "    )\n",
    "\n",
    "# Create a reusable SQLAlchemy engine that downstream cells can share.\n",
    "engine: Engine = create_engine(DATABASE_URL)\n",
    "engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b62889",
   "metadata": {},
   "source": [
    "## Extract the Latest Jobs Table Snapshot\n",
    "\n",
    "We query the core columns required for downstream analysis. Adjust the SQL to join additional tables once they are available\n",
    "(e.g., embeddings, generated resume artifacts, or recruiter contact data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13563e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_jobs(engine: Engine) -> pd.DataFrame:\n",
    "    \"\"\"Return the latest Alfred job postings needed for the network analysis.\"\"\"\n",
    "    query = text(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            id,\n",
    "            title,\n",
    "            company,\n",
    "            location,\n",
    "            description,\n",
    "            source_url,\n",
    "            match_score\n",
    "            \n",
    "        FROM jobs\n",
    "        ORDER BY id DESC;\n",
    "        \"\"\"\n",
    "    )  # Adjust the SQL when you want to join extra tables/columns.\n",
    "    return pd.read_sql_query(query, engine)\n",
    "\n",
    "\n",
    "# Pull the raw dataset once so downstream wrangling operates on an in-memory DataFrame.\n",
    "jobs_raw = fetch_jobs(engine)\n",
    "print(f\"Retrieved {len(jobs_raw):,} job postings from Alfred.\")\n",
    "jobs_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f674e4",
   "metadata": {},
   "source": [
    "## Clean Text Fields and Create Helper Columns\n",
    "\n",
    "We standardize key string columns, drop duplicate `source_url`s, and prepare a normalized description column for lightweight NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICON_PATTERN = r\"[????????????????????????????????]\"\n",
    "\n",
    "# Basic QA + normalization to reduce noisy punctuation before NLP-heavy steps.\n",
    "jobs_clean = (\n",
    "    jobs_raw.copy()\n",
    "    .dropna(subset=[\"title\", \"description\"])\n",
    "    .drop_duplicates(subset=[\"source_url\"], keep=\"first\")\n",
    "    .assign(\n",
    "        title=lambda df: df[\"title\"].str.strip(),\n",
    "        company=lambda df: df[\"company\"].fillna(\"Unknown\").str.strip(),\n",
    "        location=lambda df: df[\"location\"].fillna(\"Remote\").str.strip(),\n",
    "        description=lambda df: df[\"description\"]\n",
    "        .str.replace(\n",
    "            ICON_PATTERN, \" \", regex=True\n",
    "        )  # strip arrow/bullet icons that survive scraping\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)  # collapse multi-space/newline sequences\n",
    "        .str.strip(),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Helper columns reused in later analysis/plots.\n",
    "jobs_clean[\"description_lower\"] = jobs_clean[\"description\"].str.lower()\n",
    "jobs_clean[\"job_label\"] = jobs_clean.apply(\n",
    "    lambda row: f\"{row['title']} @ {row['company']}\", axis=1\n",
    ")\n",
    "\n",
    "print(f\"{len(jobs_clean):,} clean job postings remaining after QA.\")\n",
    "jobs_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb3436",
   "metadata": {},
   "source": [
    "** \"From CSV Files\" ** Start here to skip the database connection step. run the next cell to load the csv files directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_clean = pd.read_csv(\"artifacts/jobs_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444568b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    jobs_clean.groupby(\"title\")\n",
    "    .agg(num_roles=(\"id\", \"count\"), avg_match_score=(\"match_score\", \"mean\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"num_roles\", ascending=False)\n",
    ")\n",
    "\n",
    "summary.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2180c69",
   "metadata": {},
   "source": [
    "## Dictionary-Based Skill Extraction (Baseline)\n",
    "\n",
    "We first apply the curated keyword dictionary to tag skills, preserving the original deterministic baseline.\n",
    "These results are saved separately so we can compare coverage against the NLP-driven extraction that follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17361e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "# --- 1. CLEANED AND CORRECTED SKILL DICTIONARY ---\n",
    "SKILLS_DICTIONARY = {\n",
    "    \"Programming_Tools\": [\n",
    "        \"Python\",\n",
    "        \"R\",\n",
    "        \"SQL\",\n",
    "        \"Scala\",\n",
    "        \"Java\",\n",
    "        \"Julia\",\n",
    "        \"Bash\",\n",
    "        \"Jupyter\",\n",
    "        \"Git\",\n",
    "        \"GitHub\",\n",
    "        \"Pandas\",\n",
    "        \"NumPy\",\n",
    "        \"SciPy\",\n",
    "        \"Matplotlib\",\n",
    "        \"Seaborn\",\n",
    "        \"Plotly\",\n",
    "    ],\n",
    "    \"MLOps_DevOps_Cloud\": [\n",
    "        \"MLOps\",\n",
    "        \"Docker\",\n",
    "        \"Kubernetes\",\n",
    "        \"Airflow\",\n",
    "        \"MLflow\",\n",
    "        \"Kubeflow\",\n",
    "        \"DVC\",\n",
    "        \"AWS\",\n",
    "        \"SageMaker\",\n",
    "        \"Azure ML\",\n",
    "        \"GCP\",\n",
    "        \"BigQuery\",\n",
    "        \"S3\",\n",
    "        \"Lambda\",\n",
    "        \"Databricks\",\n",
    "        \"Snowflake\",\n",
    "        \"Spark\",\n",
    "        \"PySpark\",\n",
    "        \"Hadoop\",\n",
    "        \"Kafka\",\n",
    "    ],\n",
    "    \"Machine_Learning_and_Artificial_Intelligence\": [\n",
    "        \"Machine Learning\",\n",
    "        \"Deep Learning\",\n",
    "        \"Neural Network\",\n",
    "        \"NLP\",\n",
    "        \"Natural Language Processing\",\n",
    "        \"Computer Vision\",\n",
    "        \"CV\",\n",
    "        \"Reinforcement Learning\",\n",
    "        \"Supervised Learning\",\n",
    "        \"Unsupervised Learning\",\n",
    "        \"Semi-Supervised Learning\",\n",
    "        \"Transfer Learning\",\n",
    "        \"Time Series\",\n",
    "        \"Anomaly Detection\",\n",
    "        \"Clustering\",\n",
    "        \"Classification\",\n",
    "        \"Regression\",\n",
    "        \"Cross Validation\",\n",
    "        \"Dimensionality Reduction\",\n",
    "        \"PCA\",\n",
    "        \"t-SNE\",\n",
    "        \"UMAP\",\n",
    "        \"scikit-learn\",\n",
    "        \"sklearn\",\n",
    "        \"statsmodels\",\n",
    "        \"PyTorch\",\n",
    "        \"TensorFlow\",\n",
    "        \"Keras\",\n",
    "        \"XGBoost\",\n",
    "        \"LightGBM\",\n",
    "        \"CatBoost\",\n",
    "        \"Prophet\",\n",
    "        \"Sktime\",\n",
    "    ],\n",
    "    \"Statistics\": [\n",
    "        \"Statistics\",\n",
    "        \"Statistical Modeling\",\n",
    "        \"Hypothesis Testing\",\n",
    "        \"Bayesian Statistics\",\n",
    "        \"Confidence Intervals\",\n",
    "        \"P-Values\",\n",
    "        \"ANOVA\",\n",
    "        \"Regression Analysis\",\n",
    "        \"Chisquared Test\",\n",
    "        \"Distribution Fitting\",\n",
    "        \"Data Exploration\",\n",
    "        \"Summary Statistics\",\n",
    "        \"Descriptive Statistics\",\n",
    "    ],\n",
    "    \"Finance_Fintech_Banking\": [\n",
    "        \"Financial Markets\",\n",
    "        \"Asset Management\",\n",
    "        \"Quantitative Finance\",\n",
    "        \"Quant\",\n",
    "        \"Credit Risk\",\n",
    "        \"Market Risk\",\n",
    "        \"Operational Risk\",\n",
    "        \"Underwriting\",\n",
    "        \"Fraud Detection\",\n",
    "        \"AML\",\n",
    "        \"Anti-Money Laundering\",\n",
    "        \"KYC\",\n",
    "        \"Know Your Customer\",\n",
    "        \"Basel III\",\n",
    "        \"MiFID\",\n",
    "        \"SEC\",\n",
    "        \"Trading Algorithm\",\n",
    "        \"Algorithmic Trading\",\n",
    "        \"Fixed Income\",\n",
    "        \"Derivatives\",\n",
    "        \"Hedge Fund\",\n",
    "        \"Valuation\",\n",
    "        \"Credit Card\",\n",
    "    ],\n",
    "    \"Healthcare_Pharma_Biotech\": [\n",
    "        \"Healthtech\",\n",
    "        \"Biomedical\",\n",
    "        \"Clinical Trial\",\n",
    "        \"Drug Discovery\",\n",
    "        \"EHR\",\n",
    "        \"Electronic Health Record\",\n",
    "        \"Epic\",\n",
    "        \"Cerner\",\n",
    "        \"FHIR\",\n",
    "        \"HIPAA\",\n",
    "        \"PHI\",\n",
    "        \"Protected Health Information\",\n",
    "        \"CMS\",\n",
    "        \"Claims Data\",\n",
    "        \"ICD-10\",\n",
    "        \"CPT Codes\",\n",
    "        \"Payer\",\n",
    "        \"Provider\",\n",
    "        \"HEDIS\",\n",
    "        \"Population Health\",\n",
    "        \"Biostatistics\",\n",
    "        \"Genomics\",\n",
    "        \"Radiology\",\n",
    "        \"Medical Imaging\",\n",
    "    ],\n",
    "    \"Insurance_Risk_Underwriting\": [\n",
    "        \"Underwriting\",\n",
    "        \"Actuarial Science\",\n",
    "        \"Loss Ratio\",\n",
    "        \"Claims Analysis\",\n",
    "        \"Cat Modeling\",\n",
    "        \"Catastrophe Modeling\",\n",
    "        \"P&C\",\n",
    "        \"Property & Casualty\",\n",
    "        \"Life Insurance\",\n",
    "        \"Pricing Model\",\n",
    "        \"Reserving\",\n",
    "        \"Policy Administration\",\n",
    "        \"Regulatory Compliance\",\n",
    "        \"Solvency II\",\n",
    "        \"Capital Modeling\",\n",
    "        \"Telematics\",\n",
    "    ],\n",
    "    \"General_Business_Analytical\": [\n",
    "        \"A/B Testing\",\n",
    "        \"Experimentation\",\n",
    "        \"ROI Analysis\",\n",
    "        \"LTV\",\n",
    "        \"Customer Lifetime Value\",\n",
    "        \"Churn Prediction\",\n",
    "        \"Demand Forecasting\",\n",
    "        \"Supply Chain\",\n",
    "        \"Logistics\",\n",
    "        \"Pricing Optimization\",\n",
    "        \"Causal Inference\",\n",
    "        \"Econometrics\",\n",
    "        \"KPIs\",\n",
    "    ],\n",
    "    \"Soft_Skills_Misc\": [\n",
    "        \"Communication Skills\",\n",
    "        \"Teamwork\",\n",
    "        \"Problem Solving\",\n",
    "        \"Critical Thinking\",\n",
    "        \"Steakholder Management\",\n",
    "        \"Steakholder Engagement\",\n",
    "        \"Priotization\",\n",
    "        \"Self Starter\",\n",
    "        \"Detail Oriented\",\n",
    "        \"Start up\",\n",
    "    ],\n",
    "    \"Software_Development_Engineering\": [\n",
    "        \"API\",\n",
    "        \"REST\",\n",
    "        \"OOP\",\n",
    "        \"Data Structures\",\n",
    "        \"Algorithims\",\n",
    "        \"Microservices\",\n",
    "        \"CI/CD\",\n",
    "        \"Agile\",\n",
    "        \"Scrum\",\n",
    "        \"TDD\",\n",
    "        \"Unit Testing\",\n",
    "        \"Integration Testing\",\n",
    "        \"System Design\",\n",
    "        \"Cloud Computing\",\n",
    "        \"JavaScript\",\n",
    "        \"TypeScript\",\n",
    "        \"C++\",\n",
    "        \"C#\",\n",
    "        \"Ruby\",\n",
    "        \"PHP\",\n",
    "        \"Go\",\n",
    "        \"Swift\",\n",
    "        \"Objective-C\",\n",
    "        \"HTML\",\n",
    "        \"CSS\",\n",
    "        \"React\",\n",
    "        \"Angular\",\n",
    "        \"Vue.js\",\n",
    "        \"Node.js\",\n",
    "        \"Django\",\n",
    "        \"Flask\",\n",
    "    ],\n",
    "    \"Productivity_Tools\": [\n",
    "        \"Excel\",\n",
    "        \"Tableau\",\n",
    "        \"Power BI\",\n",
    "        \"Looker\",\n",
    "        \"Google Data Studio\",\n",
    "        \"PowerPoint\",\n",
    "        \"Word\",\n",
    "        \"Visio\",\n",
    "        \"Notion\",\n",
    "        \"Confluence\",\n",
    "        \"Slack\",\n",
    "        \"Trello\",\n",
    "        \"Asana\",\n",
    "        \"Jira\",\n",
    "        \"Google Sheets\",\n",
    "        \"Google Docs\",\n",
    "        \"Micro Soft Office\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "SKILL_PATTERNS = {\n",
    "    skill: {\n",
    "        \"category\": category,\n",
    "        \"pattern\": re.compile(rf\"\\b{re.escape(skill)}\\b\", re.IGNORECASE),\n",
    "    }\n",
    "    for category, skills in SKILLS_DICTIONARY.items()\n",
    "    for skill in skills\n",
    "}\n",
    "\n",
    "\n",
    "def extract_skills_dictionary(text: str) -> List[Tuple[str, str]]:\n",
    "    matches: List[Tuple[str, str]] = []\n",
    "    for skill, meta in SKILL_PATTERNS.items():\n",
    "        if meta[\"pattern\"].search(text):\n",
    "            matches.append((skill.title(), meta[\"category\"]))\n",
    "    return matches\n",
    "\n",
    "\n",
    "jobs_clean[\"skills_dictionary\"] = jobs_clean[\"description_lower\"].apply(\n",
    "    extract_skills_dictionary\n",
    ")\n",
    "\n",
    "jobs_skill_map_dictionary = (\n",
    "    jobs_clean[[\"id\", \"job_label\", \"company\", \"skills_dictionary\"]]\n",
    "    .explode(\"skills_dictionary\")\n",
    "    .dropna(subset=[\"skills_dictionary\"])\n",
    ")\n",
    "\n",
    "jobs_skill_map_dictionary[\"skill_name\"] = jobs_skill_map_dictionary[\n",
    "    \"skills_dictionary\"\n",
    "].apply(lambda item: item[0])\n",
    "jobs_skill_map_dictionary[\"skill_category\"] = jobs_skill_map_dictionary[\n",
    "    \"skills_dictionary\"\n",
    "].apply(lambda item: item[1])\n",
    "\n",
    "print(\n",
    "    f\"Dictionary extractor tagged {jobs_skill_map_dictionary['skill_name'].nunique():,} unique skills across {jobs_skill_map_dictionary['id'].nunique():,} jobs.\"\n",
    ")\n",
    "\n",
    "\n",
    "jobs_clean[\"skills\"] = jobs_clean[\"skills_dictionary\"].apply(\n",
    "    lambda pairs: [skill for skill, _ in pairs] if isinstance(pairs, list) else []\n",
    ")\n",
    "\n",
    "jobs_skill_map = jobs_skill_map_dictionary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a94de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bipartite job-skill graph (jobs on one partition, skills on the other).\n",
    "B = nx.Graph()\n",
    "\n",
    "# Add job nodes so each posting is uniquely identified in the graph.\n",
    "for _, row in jobs_clean.iterrows():\n",
    "    job_node = f\"job_{row['id']}\"\n",
    "    B.add_node(\n",
    "        job_node,\n",
    "        bipartite=\"job\",\n",
    "        label=row[\"job_label\"],\n",
    "        company=row[\"company\"],\n",
    "        location=row[\"location\"],\n",
    "    )\n",
    "\n",
    "# Add skill nodes and connect them to the jobs that mention them.\n",
    "for _, row in jobs_skill_map.iterrows():\n",
    "    skill_node = f\"skill_{row['skill_name'].lower().replace(' ', '_')}\"\n",
    "    B.add_node(\n",
    "        skill_node,\n",
    "        bipartite=\"skill\",\n",
    "        label=row[\"skill_name\"],\n",
    "        category=row[\"skill_category\"],\n",
    "    )\n",
    "    job_node = f\"job_{row['id']}\"\n",
    "    B.add_edge(job_node, skill_node)\n",
    "\n",
    "print(f\"Graph has {B.number_of_nodes():,} nodes and {B.number_of_edges():,} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_JOBS_FOR_BIPARTITE_PLOT = jobs_clean.shape[\n",
    "    0\n",
    "]  # Adjust this number to limit the jobs shown in the plot.\n",
    "sample_job_nodes = [\n",
    "    f\"job_{job_id}\" for job_id in jobs_clean.head(MAX_JOBS_FOR_BIPARTITE_PLOT)[\"id\"]\n",
    "]\n",
    "connected_skill_nodes = set()\n",
    "for job in sample_job_nodes:\n",
    "    connected_skill_nodes.update(B.neighbors(job))\n",
    "\n",
    "subgraph_nodes = sample_job_nodes + list(connected_skill_nodes)\n",
    "H_bipartite = B.subgraph(subgraph_nodes).copy()\n",
    "\n",
    "if not H_bipartite:\n",
    "    raise ValueError(\n",
    "        \"Bipartite subgraph is empty. Ensure jobs_skill_map is populated before plotting.\"\n",
    "    )\n",
    "\n",
    "job_nodes_sub = [\n",
    "    n for n, d in H_bipartite.nodes(data=True) if d.get(\"bipartite\") == \"job\"\n",
    "]\n",
    "skill_nodes_sub = [n for n in H_bipartite if n not in job_nodes_sub]\n",
    "\n",
    "pos = {}\n",
    "# Align jobs on the left (x=0) and skills on the right (x=1) for clarity.\n",
    "pos.update((node, (0, idx)) for idx, node in enumerate(job_nodes_sub))\n",
    "pos.update((node, (1, idx)) for idx, node in enumerate(skill_nodes_sub))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw_networkx_nodes(\n",
    "    H_bipartite,\n",
    "    pos,\n",
    "    nodelist=job_nodes_sub,\n",
    "    node_color=\"#1f3ab4f0\",\n",
    "    node_size=200,\n",
    "    label=\"Jobs\",\n",
    ")\n",
    "nx.draw_networkx_nodes(\n",
    "    H_bipartite,\n",
    "    pos,\n",
    "    nodelist=skill_nodes_sub,\n",
    "    node_color=\"#ff520e\",\n",
    "    node_size=200,\n",
    "    label=\"Skills\",\n",
    ")\n",
    "nx.draw_networkx_edges(H_bipartite, pos, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Bipartite Job >> Skill Graph\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02041118",
   "metadata": {},
   "source": [
    "Lets look at the most popular job titles in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c244d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_counts = jobs_clean[\"title\"].value_counts()\n",
    "\n",
    "title_counts.head(20)\n",
    "plt.figure()\n",
    "plt.bar(title_counts.head(20).index, title_counts.head(20).values, color=\"skyblue\")\n",
    "\n",
    "plt.title(\"Top 20 titles by Number of Job Postings\")\n",
    "plt.xlabel(\"Job Title\")\n",
    "plt.ylabel(\"Number of Job Postings\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd63321",
   "metadata": {},
   "source": [
    "Now that we see the type of job titles in the dataset, we can move forward with building the job-skill bipartite graph. lets look at top skills pulled from the job descriptions next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_counts = (\n",
    "    jobs_skill_map.groupby([\"skill_name\"])\n",
    "    .agg(num_jobs=(\"id\", \"count\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"num_jobs\", ascending=False)\n",
    ")\n",
    "\n",
    "top_skills = skill_counts.head(30)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(top_skills[\"skill_name\"], top_skills[\"num_jobs\"])\n",
    "\n",
    "plt.title(\"Top Skills  Across Alfred Jobs From Dictionary Extraction\")\n",
    "plt.xlabel(\"Skill\")\n",
    "plt.ylabel(\"Number of Jobs\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dddce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import bipartite\n",
    "\n",
    "# Separate the partitions so we can project skills onto themselves.\n",
    "skill_nodes = {n for n, d in B.nodes(data=True) if d.get(\"bipartite\") == \"skill\"}\n",
    "job_nodes = set(B) - skill_nodes\n",
    "\n",
    "# Skills are connected when they co-occur in at least one job posting (weighted by frequency).\n",
    "skill_projection = bipartite.weighted_projected_graph(B, skill_nodes)\n",
    "print(\n",
    "    f\"Skill projection has {skill_projection.number_of_nodes()} nodes / {skill_projection.number_of_edges()} edges\"\n",
    ")\n",
    "\n",
    "# Degree centrality surfaces the skills that connect to the most neighbors in this co-occurrence graph.\n",
    "centrality = nx.degree_centrality(skill_projection)\n",
    "centrality_df = pd.DataFrame(\n",
    "    {\n",
    "        \"skill_node\": list(skill_projection.nodes()),\n",
    "        \"skill_name\": [\n",
    "            skill_projection.nodes[n][\"label\"] for n in skill_projection.nodes\n",
    "        ],\n",
    "        \"category\": [\n",
    "            skill_projection.nodes[n][\"category\"] for n in skill_projection.nodes\n",
    "        ],\n",
    "        \"centrality\": [centrality[n] for n in skill_projection.nodes],\n",
    "    }\n",
    ").sort_values(\"centrality\", ascending=False)\n",
    "\n",
    "centrality_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a965a",
   "metadata": {},
   "source": [
    "Lets see if we can visualize the top 40 nodes by their degree centrality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on the most central skills so the visualization stays interpretable.\n",
    "top_nodes = centrality_df.head(40)[\"skill_node\"]\n",
    "H = skill_projection.subgraph(top_nodes).copy()\n",
    "pos = nx.spring_layout(H, seed=42)  # deterministic layout for reproducibility\n",
    "nodes = list(H.nodes())\n",
    "x = [pos[n][0] for n in nodes]\n",
    "y = [pos[n][1] for n in nodes]\n",
    "size = [10 + 80 * centrality[n] for n in nodes]\n",
    "categories = pd.Categorical([H.nodes[n][\"category\"] for n in nodes])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# edges as line segments\n",
    "for u, v, data in H.edges(data=True):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[pos[u][0], pos[v][0]],\n",
    "            y=[pos[u][1], pos[v][1]],\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=max(1, data.get(\"weight\", 1) * 0.2), color=\"#cccccc\"),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# nodes\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode=\"markers+text\",\n",
    "        text=[H.nodes[n][\"label\"] for n in nodes],\n",
    "        textposition=\"bottom center\",\n",
    "        marker=dict(\n",
    "            size=size,\n",
    "            color=categories.codes,\n",
    "            colorscale=\"Turbo\",\n",
    "            line=dict(width=1, color=\"#333\"),\n",
    "        ),\n",
    "        hovertext=[\n",
    "            f\"{H.nodes[n]['label']}<br>Category: {H.nodes[n]['category']}<br>Centrality: {centrality[n]:.3f}\"\n",
    "            for n in nodes\n",
    "        ],\n",
    "        hoverinfo=\"text\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Top 40 Skills by Degree of Centrality Co-occurrence Network\",\n",
    "    xaxis=dict(visible=False),\n",
    "    yaxis=dict(visible=False),\n",
    "    showlegend=False,\n",
    "    margin=dict(l=20, r=20, t=60, b=20),\n",
    ")\n",
    "fig.show()\n",
    "fig.to_html(\"artifacts/skill_cooccurrence_network.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706e1f8",
   "metadata": {},
   "source": [
    "## Persist Intermediate Tables (Optional)\n",
    "\n",
    "- You don't need to run this cell if you are starting from the CSV files above.\n",
    "  Export clean job and job/skill mapping tables so they can be fed into Gephi or downstream dashboards without\n",
    "  re-querying the live database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ee8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist tidy tables so collaborators can use them without re-running the database queries.\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Parquet for efficient reloads.\n",
    "jobs_clean.to_parquet(OUTPUT_DIR / \"jobs_clean.parquet\", index=False)\n",
    "jobs_skill_map.to_parquet(OUTPUT_DIR / \"jobs_skill_map.parquet\", index=False)\n",
    "\n",
    "# CSV exports for easy sharing/export to other tools.\n",
    "jobs_clean.to_csv(OUTPUT_DIR / \"jobs_clean.csv\", index=False)\n",
    "jobs_skill_map.to_csv(OUTPUT_DIR / \"jobs_skill_map.csv\", index=False)\n",
    "\n",
    "print(f\"Saved artifacts to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8667283",
   "metadata": {},
   "source": [
    "Now we can see if we detect any communities among the top skills using the Louvain method.\n",
    "The Louvain method is a popular algorithm for community detection in large networks.\n",
    "It optimizes modularity to find clusters of nodes that are more densely connected internally than with the rest of the network.\n",
    "It does this through a two-phase process: first, it assigns each node to its own community and iteratively merges communities to maximize modularity; second, it builds a new network where nodes represent the detected communities and repeats the process until no further modularity improvement is possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e31bc3",
   "metadata": {},
   "source": [
    "## Skill \"Island\" Communities\n",
    "\n",
    "To surface densely connected pockets of skills, we threshold the skill-projection edges by weight and\n",
    "run a modularity-based community detector. Tweak the `WEIGHT_THRESHOLD` or drop the filter entirely\n",
    "if you want to study the full graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [data[\"weight\"] for _, _, data in skill_projection.edges(data=True)]\n",
    "min_weight = min(weights)\n",
    "max_weight = max(weights)\n",
    "print(min_weight, max_weight)\n",
    "\n",
    "\n",
    "WEIGHT_THRESHOLD = 3  # minimum shared-job weight required to keep an edge\n",
    "\n",
    "filtered_edges = [\n",
    "    (u, v)\n",
    "    for u, v, data in skill_projection.edges(data=True)\n",
    "    if data.get(\"weight\", 0) >= WEIGHT_THRESHOLD\n",
    "]\n",
    "\n",
    "if not filtered_edges:\n",
    "    raise ValueError(\n",
    "        \"No edges survived the weight threshold. Adjust WEIGHT_THRESHOLD or refine the NLP skill extraction heuristics.\"\n",
    "    )\n",
    "\n",
    "# edge_subgraph preserves node attributes (label/category) from the original skill projection\n",
    "island_graph = skill_projection.edge_subgraph(filtered_edges).copy()\n",
    "\n",
    "communities = list(nx_comm.greedy_modularity_communities(island_graph, weight=\"weight\"))\n",
    "community_map = {\n",
    "    node: idx for idx, nodes in enumerate(communities, start=1) for node in nodes\n",
    "}\n",
    "\n",
    "community_summary = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"community_id\": idx,\n",
    "            \"size\": len(nodes),\n",
    "            \"sample_skills\": \", \".join(\n",
    "                sorted(island_graph.nodes[n][\"label\"] for n in list(nodes)[:5])\n",
    "            ),\n",
    "        }\n",
    "        for idx, nodes in enumerate(communities, start=1)\n",
    "    ]\n",
    ").sort_values(\"size\", ascending=False)\n",
    "\n",
    "print(f\"Detected {len(communities)} dense skill communities after thresholding.\")\n",
    "community_summary.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize islands with Plotly for interactive inspection\n",
    "H = island_graph  # already contains filtered nodes with attributes\n",
    "pos = nx.spring_layout(H, seed=42, k=0.8)\n",
    "nodes = list(H.nodes())\n",
    "node_x = [pos[n][0] for n in nodes]\n",
    "node_y = [pos[n][1] for n in nodes]\n",
    "node_sizes = [12 + 30 * centrality.get(n, 0) for n in nodes]\n",
    "communities_cat = pd.Categorical([community_map.get(n, 0) for n in nodes])\n",
    "\n",
    "fig = go.Figure()\n",
    "for u, v, data in H.edges(data=True):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[pos[u][0], pos[v][0]],\n",
    "            y=[pos[u][1], pos[v][1]],\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=max(1, data.get(\"weight\", 1) * 0.15), color=\"#bbbbbb\"),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode=\"markers+text\",\n",
    "        text=[H.nodes[n][\"label\"] for n in nodes],\n",
    "        textposition=\"bottom center\",\n",
    "        marker=dict(\n",
    "            size=node_sizes,\n",
    "            color=communities_cat.codes,\n",
    "            colorscale=\"Plasma\",\n",
    "            line=dict(width=1, color=\"#333\"),\n",
    "        ),\n",
    "        hovertext=[\n",
    "            f\"{H.nodes[n]['label']}<br>Community: {community_map.get(n)}<br>Degree: {H.degree(n)}\"\n",
    "            for n in nodes\n",
    "        ],\n",
    "        hoverinfo=\"text\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Skill Islands (communities on weighted projection)\",\n",
    "    xaxis=dict(visible=False),\n",
    "    yaxis=dict(visible=False),\n",
    "    margin=dict(l=20, r=20, t=60, b=20),\n",
    ")\n",
    "fig.write_html(\"skill_islands_network.html\", auto_open=True)\n",
    "fig.write_json(\"skill_islands_visualization.json\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c440408",
   "metadata": {},
   "source": [
    "## Dyanmic -Analysis NLP Skill Extraction\n",
    "\n",
    "After completing the dictionary-driven network analysis above, we can optionally\n",
    "generate a dynamic skill inventory with spaCy to compare coverage. This cell works on a copy of\n",
    "`jobs_clean` so the baseline results stay intact.\n",
    "\n",
    "We use spaCy's PhraseMatcher to identify skill phrases in job descriptions.\n",
    "We then rebuild the job-skill bipartite graph using the NLP-extracted skills and repeat the\n",
    "centrality and community detection analyses to compare against the baseline.\n",
    "We add some stopword skills to filter out common but uninformative terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e1b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy.cli import download as spacy_download\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# make a copy of jobs_clean to avoid modifying the baseline skill extraction results\n",
    "jobs_clean_nlp = jobs_clean.copy().reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    spacy_download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Adjust spaCy's max_length if any job description exceeds the default limit.\n",
    "max_desc_length = int(jobs_clean_nlp[\"description\"].fillna(\"\").str.len().max())\n",
    "if max_desc_length:\n",
    "    nlp.max_length = max(nlp.max_length, int(max_desc_length * 1.2))\n",
    "# Define custom stopwords and filtering heuristics.\n",
    "GENERIC_TERMS = {\n",
    "    \"ability\",\n",
    "    \"abilities\",\n",
    "    \"team\",\n",
    "    \"teams\",\n",
    "    \"company\",\n",
    "    \"companies\",\n",
    "    \"business\",\n",
    "    \"clients\",\n",
    "    \"client\",\n",
    "    \"stakeholders\",\n",
    "    \"stakeholder\",\n",
    "    \"role\",\n",
    "    \"roles\",\n",
    "    \"position\",\n",
    "    \"positions\",\n",
    "    \"responsibilities\",\n",
    "    \"responsibility\",\n",
    "    \"experience\",\n",
    "    \"experiences\",\n",
    "    \"people\",\n",
    "    \"solutions\",\n",
    "    \"services\",\n",
    "    \"product\",\n",
    "    \"products\",\n",
    "    \"organization\",\n",
    "    \"organizations\",\n",
    "    \"environment\",\n",
    "    \"environments\",\n",
    "    \"technology\",\n",
    "    \"technologies\",\n",
    "    \"year\",\n",
    "    \"years\",\n",
    "    \"degree\",\n",
    "    \"degrees\",\n",
    "    \"applicant\",\n",
    "    \"applicants\",\n",
    "    \"job\",\n",
    "    \"jobs\",\n",
    "    \"candidate\",\n",
    "    \"candidates\",\n",
    "    \"orientation\",\n",
    "    \"Data Engineer\",\n",
    "    \"Data Scientist\",\n",
    "}\n",
    "# Words that are unlikely to be part of a skill phrase when they appear at the end.\n",
    "BAD_ENDINGS = {\n",
    "    \"team\",\n",
    "    \"teams\",\n",
    "    \"company\",\n",
    "    \"business\",\n",
    "    \"clients\",\n",
    "    \"stakeholders\",\n",
    "    \"organization\",\n",
    "    \"environment\",\n",
    "    \"technology\",\n",
    "    \"year\",\n",
    "    \"job\",\n",
    "}\n",
    "\n",
    "# some custom stopwords relevant to job descriptions\n",
    "CUSTOM_STOPWORDS = {\n",
    "    \"technology\",\n",
    "    \"technologies\",\n",
    "    \"year\",\n",
    "    \"years\",\n",
    "    \"degree\",\n",
    "    \"degrees\",\n",
    "    \"applicant\",\n",
    "    \"applicants\",\n",
    "    \"job\",\n",
    "    \"jobs\",\n",
    "    \"candidate\",\n",
    "    \"candidates\",\n",
    "    \"orientation\",\n",
    "    \"team\",\n",
    "    \"teams\",\n",
    "    \"company\",\n",
    "    \"business\",\n",
    "    \"people\",\n",
    "    \"Title\",\n",
    "}\n",
    "STOPWORD_SET = {word.lower() for word in STOP_WORDS}.union(CUSTOM_STOPWORDS)\n",
    "\n",
    "DICTIONARY_SINGLE_SKILLS = {\n",
    "    skill.lower() for skills in SKILLS_DICTIONARY.values() for skill in skills\n",
    "}\n",
    "SINGLE_SKILL_ALLOWLIST = DICTIONARY_SINGLE_SKILLS.union(\n",
    "    {\n",
    "        \"data\",\n",
    "        \"analytics\",\n",
    "        \"analysis\",\n",
    "        \"integration\",\n",
    "        \"engineering\",\n",
    "        \"modeling\",\n",
    "        \"machine\",\n",
    "        \"learning\",\n",
    "        \"ai\",\n",
    "        \"ml\",\n",
    "        \"python\",\n",
    "        \"sql\",\n",
    "        \"spark\",\n",
    "        \"aws\",\n",
    "        \"azure\",\n",
    "        \"gcp\",\n",
    "    }\n",
    ")\n",
    "ANCHOR_TERMS = {\n",
    "    \"data\",\n",
    "    \"analytics\",\n",
    "    \"analysis\",\n",
    "    \"integration\",\n",
    "    \"science\",\n",
    "    \"engineering\",\n",
    "    \"machine\",\n",
    "    \"learning\",\n",
    "    \"intelligence\",\n",
    "    \"modeling\",\n",
    "    \"ai\",\n",
    "    \"ml\",\n",
    "    \"cloud\",\n",
    "    \"platform\",\n",
    "    \"product\",\n",
    "    \"pipeline\",\n",
    "    \"governance\",\n",
    "    \"visualization\",\n",
    "    \"automation\",\n",
    "    \"insight\",\n",
    "    \"insights\",\n",
    "}\n",
    "\n",
    "\n",
    "# Normalization function to clean and standardize phrases.\n",
    "def normalize_phrase(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9+#/&\\- ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Token filtering functions.\n",
    "def token_is_candidate(token) -> bool:\n",
    "    if token.is_stop or token.is_punct:\n",
    "        return False\n",
    "    lemma = token.lemma_.lower()\n",
    "    if lemma in STOPWORD_SET:\n",
    "        return False\n",
    "    return token.pos_ in {\"PROPN\", \"NOUN\", \"ADJ\"}\n",
    "\n",
    "\n",
    "# Heuristic to allow certain single-token skills.\n",
    "def allow_single_token(token, norm: str) -> bool:\n",
    "    lemma = token.lemma_.lower()\n",
    "    text = token.text\n",
    "    if lemma in SINGLE_SKILL_ALLOWLIST:\n",
    "        return True\n",
    "    if text.isupper() and len(text) <= 4:\n",
    "        return True\n",
    "    if any(char.isdigit() for char in text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Heuristic to filter out non-skill-like phrases.\n",
    "def is_skill_like(text: str) -> bool:\n",
    "    if not text or len(text) < 3:\n",
    "        return False\n",
    "    words = text.split()\n",
    "    if len(words) > 4:\n",
    "        return False\n",
    "    if all(word in GENERIC_TERMS for word in words):\n",
    "        return False\n",
    "    if words[-1] in BAD_ENDINGS:\n",
    "        return False\n",
    "    if text in STOPWORD_SET:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Main extraction function for a spaCy doc.\n",
    "def extract_skill_candidates(doc):\n",
    "    phrases = set()\n",
    "    candidate_tokens = [token for token in doc if token_is_candidate(token)]\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        norm = normalize_phrase(chunk.text)\n",
    "        if not is_skill_like(norm):\n",
    "            continue\n",
    "        words = norm.split()\n",
    "        if len(words) == 1:\n",
    "            token = chunk.root\n",
    "            if allow_single_token(token, norm):\n",
    "                phrases.add(norm)\n",
    "        else:\n",
    "            if any(word in ANCHOR_TERMS for word in words):\n",
    "                phrases.add(norm)\n",
    "    # Single-token candidates\n",
    "    for token in candidate_tokens:\n",
    "        norm = normalize_phrase(token.text)\n",
    "        if is_skill_like(norm) and allow_single_token(token, norm):\n",
    "            phrases.add(norm)\n",
    "    # Multi-token candidates (2-3 grams)\n",
    "    max_window = 3\n",
    "    for window in range(2, max_window + 1):\n",
    "        for i in range(len(candidate_tokens) - window + 1):\n",
    "            phrase_tokens = candidate_tokens[i : i + window]\n",
    "            lemma_window = [token.lemma_.lower() for token in phrase_tokens]\n",
    "            if not any(lemma in ANCHOR_TERMS for lemma in lemma_window):\n",
    "                continue\n",
    "            phrase = \" \".join(token.text for token in phrase_tokens)\n",
    "            norm = normalize_phrase(phrase)\n",
    "            if is_skill_like(norm):\n",
    "                phrases.add(norm)\n",
    "\n",
    "    return sorted(phrases)\n",
    "\n",
    "\n",
    "# Extract skill candidates from each job description.\n",
    "descriptions = jobs_clean_nlp[\"description\"].fillna(\"\")\n",
    "skill_candidates = []\n",
    "for doc in tqdm(\n",
    "    nlp.pipe(descriptions.tolist(), batch_size=32),\n",
    "    total=len(descriptions),\n",
    "    desc=\"Extracting skill candidates\",\n",
    "):\n",
    "    skill_candidates.append(extract_skill_candidates(doc))\n",
    "\n",
    "jobs_clean_nlp[\"skill_candidates_raw\"] = skill_candidates\n",
    "\n",
    "candidate_counts = Counter(itertools.chain.from_iterable(skill_candidates))\n",
    "if not candidate_counts:\n",
    "    raise ValueError(\"spaCy-based skill extraction did not produce any candidates.\")\n",
    "# Apply global frequency filtering to build the skill vocabulary.\n",
    "MIN_GLOBAL_FREQ = 5\n",
    "MAX_GLOBAL_SHARE = 0.65\n",
    "max_allowed = max(1, int(MAX_GLOBAL_SHARE * len(jobs_clean_nlp)))\n",
    "skill_vocabulary = {\n",
    "    phrase\n",
    "    for phrase, count in candidate_counts.items()\n",
    "    if count >= MIN_GLOBAL_FREQ and count <= max_allowed and phrase not in STOPWORD_SET\n",
    "}\n",
    "\n",
    "\n",
    "# Finalize skill list for each job by filtering to the vocabulary and deduplicating.\n",
    "def finalize_skills(candidate_list):\n",
    "    filtered = [phrase for phrase in candidate_list if phrase in skill_vocabulary]\n",
    "    return sorted(set(filtered))\n",
    "\n",
    "\n",
    "# Apply finalization to each job's candidates.\n",
    "jobs_clean_nlp[\"skills_nlp\"] = [\n",
    "    [phrase.title() for phrase in finalize_skills(candidates)]\n",
    "    for candidates in skill_candidates\n",
    "]\n",
    "# Build the job-skill mapping DataFrame from NLP-extracted skills.\n",
    "jobs_skill_map_dynamic = (\n",
    "    jobs_clean_nlp[[\"id\", \"job_label\", \"company\", \"skills_nlp\"]]\n",
    "    .explode(\"skills_nlp\")\n",
    "    .dropna(subset=[\"skills_nlp\"])\n",
    "    .rename(columns={\"skills_nlp\": \"skill_name\"})\n",
    ")\n",
    "# Ensure we have some skills after filtering.\n",
    "if jobs_skill_map_dynamic.empty:\n",
    "    raise ValueError(\"No NLP-derived skills survived the filtering heuristics.\")\n",
    "# Prepare for clustering by normalizing skill names.\n",
    "jobs_skill_map_dynamic[\"skill_name\"] = jobs_skill_map_dynamic[\"skill_name\"].str.strip()\n",
    "\n",
    "# Cluster similar skills using TF-IDF + MiniBatchKMeans.\n",
    "unique_skills = jobs_skill_map_dynamic[\"skill_name\"].str.lower().unique().tolist()\n",
    "skill_category_lookup = {}\n",
    "if unique_skills:\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "    skill_matrix = vectorizer.fit_transform(unique_skills)\n",
    "    n_clusters = min(8, len(unique_skills))\n",
    "    if n_clusters == 1:\n",
    "        cluster_labels = [0]\n",
    "    else:\n",
    "        clusterer = MiniBatchKMeans(\n",
    "            n_clusters=n_clusters,\n",
    "            random_state=42,\n",
    "            batch_size=min(256, len(unique_skills)),\n",
    "            max_iter=200,\n",
    "        )\n",
    "        cluster_labels = clusterer.fit_predict(skill_matrix)\n",
    "    skill_category_lookup = {\n",
    "        skill: f\"NLP Cluster {label + 1}\"\n",
    "        for skill, label in zip(unique_skills, cluster_labels)\n",
    "    }\n",
    "# Map clustered categories back to the job-skill mapping.\n",
    "jobs_skill_map_dynamic[\"skill_category\"] = (\n",
    "    jobs_skill_map_dynamic[\"skill_name\"].str.lower().map(skill_category_lookup)\n",
    ")\n",
    "jobs_skill_map_dynamic[\"skill_category\"] = jobs_skill_map_dynamic[\n",
    "    \"skill_category\"\n",
    "].fillna(\"NLP Cluster 1\")\n",
    "jobs_skill_map_dynamic[\"skill_strength\"] = (\n",
    "    jobs_skill_map_dynamic[\"skill_name\"].str.lower().map(candidate_counts)\n",
    ")\n",
    "\n",
    "# Summary of NLP-extracted skills.\n",
    "print(\n",
    "    f\"NLP extractor tagged {jobs_skill_map_dynamic['skill_name'].nunique():,} unique skills across {jobs_skill_map_dynamic['id'].nunique():,} jobs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_extraction_summary = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"method\": \"Dictionary\",\n",
    "            \"unique_skills\": jobs_skill_map_dictionary[\"skill_name\"].nunique(),\n",
    "            \"jobs_with_skills\": jobs_skill_map_dictionary[\"id\"].nunique(),\n",
    "        },\n",
    "        {\n",
    "            \"method\": \"NLP\",\n",
    "            \"unique_skills\": jobs_skill_map_dynamic[\"skill_name\"].nunique(),\n",
    "            \"jobs_with_skills\": jobs_skill_map_dynamic[\"id\"].nunique(),\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "skill_extraction_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a0d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_frequency = (\n",
    "    jobs_skill_map_dynamic[\"skill_name\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"skill_name\")\n",
    "    .reset_index(name=\"mention_count\")\n",
    ")\n",
    "skill_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e539ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "custom_stopwords = {\n",
    "    \"technology\",\n",
    "    \"technologies\",\n",
    "    \"year\",\n",
    "    \"years\",\n",
    "    \"degree\",\n",
    "    \"degrees\",\n",
    "    \"applicant\",\n",
    "    \"applicants\",\n",
    "    \"job\",\n",
    "    \"jobs\",\n",
    "    \"candidate\",\n",
    "    \"candidates\",\n",
    "    \"orientation\",\n",
    "    \"team\",\n",
    "    \"teams\",\n",
    "    \"company\",\n",
    "    \"business\",\n",
    "    \"people\",\n",
    "    \"Excellent\",\n",
    "    \"Which\",\n",
    "    \"Must\",\n",
    "    \"Will\",\n",
    "    \"Work\",\n",
    "    \"Using\",\n",
    "    \"knowledge\",\n",
    "    \"skills\",\n",
    "    \"high\",\n",
    "    \"engineer\",\n",
    "    \"Science\",\n",
    "    \"an employee\",\n",
    "    \"aware\",\n",
    "    \"Service\",\n",
    "    \"Services\",\n",
    "    \"Based\",\n",
    "    \"Level\",\n",
    "    \"Levels\",\n",
    "    \"Various\",\n",
    "    \"including\",\n",
    "    \"related\",\n",
    "    \"field\",\n",
    "    \"fields\",\n",
    "    \"areas\",\n",
    "    \"area\",\n",
    "    \"Entreprise\",\n",
    "    \"enterprises\",\n",
    "    \"The role\",\n",
    "    \"ability\",\n",
    "    \"abilities\",\n",
    "    \"stakeholder\",\n",
    "    \"stakeholders\",\n",
    "    \"data\",\n",
    "    \"organization\",\n",
    "    \"organizations\",\n",
    "    \"environment\",\n",
    "    \"environments\",\n",
    "    \"range\",\n",
    "    \"the range\",\n",
    "    \"life\",\n",
    "    \"lives\",\n",
    "    \"the life\",\n",
    "    \"datum\",\n",
    "    \"platform\",\n",
    "    \"engineering\",\n",
    "    \"new\",\n",
    "    \"design\",\n",
    "    \"analysts\",\n",
    "    \"analyst\",\n",
    "    \"functional\",\n",
    "    \"industry\",\n",
    "    \"problem\",\n",
    "    \"tool\",\n",
    "    \"decision\",\n",
    "    \"application\",\n",
    "    \"dtrong\",\n",
    "    \"day\",\n",
    "    \"cross\",\n",
    "    \"news\",\n",
    "    \"development\",\n",
    "    \"this role\",\n",
    "    \"solution\",\n",
    "    \"time\",\n",
    "    \"world\",\n",
    "    \"system\",\n",
    "    \"systems\",\n",
    "    \"opportunity\",\n",
    "    \"opportunities\",\n",
    "    \"technical\",\n",
    "    \"skill\",\n",
    "    \"software\",\n",
    "    \"quality\",\n",
    "    \"customers\",\n",
    "    \"customer\",\n",
    "    \"location\",\n",
    "    \"locations\",\n",
    "    \"process\",\n",
    "    \"impact\",\n",
    "    \"impacts\",\n",
    "    \"member\",\n",
    "    \"description\",\n",
    "    \"project\",\n",
    "    \"strong\",\n",
    "    \"benefit\",\n",
    "    \"analysis\",\n",
    "    \"support\",\n",
    "    \"enterprise\",\n",
    "    \"enterprises\",\n",
    "    \"management\",\n",
    "    \"employee\",\n",
    "    \"information\",\n",
    "    \"informations\",\n",
    "    \"end\",\n",
    "    \"lead\",\n",
    "    \"employment\",\n",
    "    \"work\",\n",
    "    \"works\",\n",
    "    \"initiative\",\n",
    "    \"requirement\",\n",
    "    \"qualification\",\n",
    "    \"qualifications\",\n",
    "    \"complex\",\n",
    "    \"complexity\",\n",
    "    \"collaborate\",\n",
    "    \"collaborates\",\n",
    "    \"collaboration\",\n",
    "    \"disability\",\n",
    "    \"diversity\",\n",
    "    \"innovation\",\n",
    "    \"innovative\",\n",
    "    \"infrastructure\",\n",
    "    \"expertise\",\n",
    "    \"performance\",\n",
    "    \"performances\",\n",
    "    \"methodology\",\n",
    "    \"methodologies\",\n",
    "    \"method\",\n",
    "    \"methods\",\n",
    "    \"status\",\n",
    "    \"euality\",\n",
    "    \"equal\",\n",
    "    \"employer\",\n",
    "    \"employers\",\n",
    "    \"global\",\n",
    "    \"analytics\",\n",
    "    \"analytic\",\n",
    "    \"individual\",\n",
    "    \"Office\",\n",
    "    \"401 K\",\n",
    "    \"U S\",\n",
    "    \"USD\",\n",
    "}\n",
    "custom_stopwords = {word.lower() for word in custom_stopwords}\n",
    "stopword_flags = skill_frequency.assign(\n",
    "    skill_lower=lambda df: df[\"skill_name\"].str.lower(),\n",
    "    spacy_stop=lambda df: df[\"skill_lower\"].isin(STOP_WORDS),\n",
    "    custom_stop=lambda df: df[\"skill_lower\"].isin(custom_stopwords),\n",
    ")\n",
    "stopword_flags[[\"skill_name\", \"mention_count\", \"spacy_stop\", \"custom_stop\"]]\n",
    "stopword_flags_filtered = stopword_flags[\n",
    "    ~(stopword_flags[\"spacy_stop\"] | stopword_flags[\"custom_stop\"])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "stopword_flags_filtered.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c85812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter NLP-derived skills to drop stop words before ranking\n",
    "stopword_set = {word.lower() for word in STOP_WORDS}.union(custom_stopwords)\n",
    "jobs_skill_map_dynamic = jobs_skill_map_dynamic[\n",
    "    ~jobs_skill_map_dynamic[\"skill_name\"].str.lower().isin(stopword_set)\n",
    "].copy()\n",
    "skill_frequency = (\n",
    "    jobs_skill_map_dynamic[\"skill_name\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"skill_name\")\n",
    "    .reset_index(name=\"mention_count\")\n",
    ")\n",
    "skill_frequency.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a867e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_freq_ranked = (\n",
    "    skill_frequency.sort_values(\"mention_count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"skill_rank\"})\n",
    ")\n",
    "fig = px.bar(\n",
    "    skill_freq_ranked,\n",
    "    x=\"skill_rank\",\n",
    "    y=\"mention_count\",\n",
    "    title=\"Skill Mention Frequency (NLP Extraction)\",\n",
    "    labels={\"skill_rank\": \"Skill Rank\", \"mention_count\": \"Mentions\"},\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"Rank %{x}<br>Skill: %{customdata[0]}<br>Mentions: %{y}\",\n",
    "    customdata=skill_freq_ranked[[\"skill_name\"]].values,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f38e96",
   "metadata": {},
   "source": [
    "## NLP Skill Graphs\n",
    "\n",
    "Reconstruct the job-skill network using the NLP-derived skills and compare it against the dictionary-based graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the NLP-based bipartite graph and projected skill network\n",
    "try:\n",
    "    jobs_skill_map_dynamic\n",
    "    jobs_clean_nlp\n",
    "except NameError as exc:  # pragma: no cover\n",
    "    raise RuntimeError(\"Run the NLP extraction cells before this section.\") from exc\n",
    "# Build the bipartite job-skill graph (jobs on one partition, skills on the other).\n",
    "B_nlp = nx.Graph()\n",
    "for _, row in jobs_clean_nlp.iterrows():\n",
    "    job_node = f\"job_{row['id']}\"\n",
    "    B_nlp.add_node(\n",
    "        job_node,\n",
    "        bipartite=\"job\",\n",
    "        label=row.get(\"job_label\", row.get(\"title\", \"Job\")),\n",
    "        company=row.get(\"company\", \"Unknown\"),\n",
    "        location=row.get(\"location\", \"Unknown\"),\n",
    "    )\n",
    "# Add skill nodes and connect them to the jobs that mention them.\n",
    "for _, row in jobs_skill_map_dynamic.iterrows():\n",
    "    job_node = f\"job_{row['id']}\"\n",
    "    skill_node = f\"skill_{row['skill_name'].lower().replace(' ', '_')}\"\n",
    "    B_nlp.add_node(\n",
    "        skill_node,\n",
    "        bipartite=\"skill\",\n",
    "        label=row[\"skill_name\"],\n",
    "        category=row.get(\"skill_category\", \"NLP\"),\n",
    "    )\n",
    "    if B_nlp.has_node(job_node):\n",
    "        B_nlp.add_edge(job_node, skill_node)\n",
    "# Ensure the graph is not empty.\n",
    "if B_nlp.number_of_nodes() == 0:\n",
    "    raise ValueError(\n",
    "        \"NLP bipartite graph is empty. Ensure jobs_skill_map_dynamic is populated.\"\n",
    "    )\n",
    "# Summarize the NLP bipartite graph.\n",
    "skill_nodes_nlp = {\n",
    "    n for n, d in B_nlp.nodes(data=True) if d.get(\"bipartite\") == \"skill\"\n",
    "}\n",
    "job_nodes_nlp = set(B_nlp) - skill_nodes_nlp\n",
    "print(\n",
    "    f\"B_nlp contains {len(job_nodes_nlp):,} job nodes, {len(skill_nodes_nlp):,} skill nodes, and {B_nlp.number_of_edges():,} edges.\"\n",
    ")\n",
    "# Project skills onto themselves based on co-occurrence in job postings.\n",
    "skill_projection_nlp = bipartite.weighted_projected_graph(B_nlp, skill_nodes_nlp)\n",
    "print(\n",
    "    f\"NLP skill projection has {skill_projection_nlp.number_of_nodes():,} nodes / {skill_projection_nlp.number_of_edges():,} edges\"\n",
    ")\n",
    "# Compute degree centrality on the NLP skill projection.\n",
    "centrality_nlp = nx.degree_centrality(skill_projection_nlp)\n",
    "centrality_df_nlp = pd.DataFrame(\n",
    "    {\n",
    "        \"skill_node\": list(skill_projection_nlp.nodes()),\n",
    "        \"skill_name\": [\n",
    "            skill_projection_nlp.nodes[n][\"label\"] for n in skill_projection_nlp.nodes\n",
    "        ],\n",
    "        \"category\": [\n",
    "            skill_projection_nlp.nodes[n].get(\"category\", \"NLP\")\n",
    "            for n in skill_projection_nlp.nodes\n",
    "        ],\n",
    "        \"centrality\": [centrality_nlp[n] for n in skill_projection_nlp.nodes],\n",
    "    }\n",
    ").sort_values(\"centrality\", ascending=False)\n",
    "\n",
    "centrality_df_nlp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_projection(graph, skill_nodes_count, job_nodes_count, projection):\n",
    "    degree_values = [deg for _, deg in projection.degree()]\n",
    "    avg_degree = (sum(degree_values) / len(degree_values)) if degree_values else 0\n",
    "    return {\n",
    "        \"jobs\": job_nodes_count,\n",
    "        \"skills\": skill_nodes_count,\n",
    "        \"job_skill_edges\": graph.number_of_edges(),\n",
    "        \"projection_nodes\": projection.number_of_nodes(),\n",
    "        \"projection_edges\": projection.number_of_edges(),\n",
    "        \"projection_avg_degree\": avg_degree,\n",
    "    }\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"pipeline\": \"Dictionary\",\n",
    "            **summarize_projection(\n",
    "                B, len(skill_nodes), len(job_nodes), skill_projection\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"pipeline\": \"NLP\",\n",
    "            **summarize_projection(\n",
    "                B_nlp, len(skill_nodes_nlp), len(job_nodes_nlp), skill_projection_nlp\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_centrality = pd.concat(\n",
    "    [\n",
    "        centrality_df.head(top_k).assign(pipeline=\"Dictionary\"),\n",
    "        centrality_df_nlp.head(top_k).assign(pipeline=\"NLP\"),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "top_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_JOBS_FOR_NLP_BIPARTITE_PLOT = jobs_clean_nlp.shape[0]\n",
    "sample_job_nodes_nlp = [\n",
    "    f\"job_{job_id}\"\n",
    "    for job_id in jobs_clean_nlp.head(MAX_JOBS_FOR_NLP_BIPARTITE_PLOT)[\"id\"]\n",
    "]\n",
    "connected_skill_nodes_nlp = set()\n",
    "for job in sample_job_nodes_nlp:\n",
    "    if B_nlp.has_node(job):\n",
    "        connected_skill_nodes_nlp.update(B_nlp.neighbors(job))\n",
    "\n",
    "subgraph_nodes_nlp = sample_job_nodes_nlp + list(connected_skill_nodes_nlp)\n",
    "H_bipartite_nlp = B_nlp.subgraph(subgraph_nodes_nlp).copy()\n",
    "\n",
    "if not H_bipartite_nlp:\n",
    "    raise ValueError(\n",
    "        \"NLP bipartite subgraph is empty. Ensure jobs_skill_map_dynamic is populated before plotting.\"\n",
    "    )\n",
    "\n",
    "job_nodes_sub_nlp = [\n",
    "    n for n, d in H_bipartite_nlp.nodes(data=True) if d.get(\"bipartite\") == \"job\"\n",
    "]\n",
    "skill_nodes_sub_nlp = [n for n in H_bipartite_nlp if n not in job_nodes_sub_nlp]\n",
    "\n",
    "pos_nlp = {}\n",
    "pos_nlp.update((node, (0, idx)) for idx, node in enumerate(job_nodes_sub_nlp))\n",
    "pos_nlp.update((node, (1, idx)) for idx, node in enumerate(skill_nodes_sub_nlp))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw_networkx_nodes(\n",
    "    H_bipartite_nlp,\n",
    "    pos_nlp,\n",
    "    nodelist=job_nodes_sub_nlp,\n",
    "    node_color=\"#1f3ab4f0\",\n",
    "    node_size=200,\n",
    "    label=\"Jobs\",\n",
    ")\n",
    "nx.draw_networkx_nodes(\n",
    "    H_bipartite_nlp,\n",
    "    pos_nlp,\n",
    "    nodelist=skill_nodes_sub_nlp,\n",
    "    node_color=\"#ff520e\",\n",
    "    node_size=200,\n",
    "    label=\"Skills\",\n",
    ")\n",
    "nx.draw_networkx_edges(H_bipartite_nlp, pos_nlp, alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"NLP Bipartite Job >> Skill Graph\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes_nlp = centrality_df_nlp.head(100)[\"skill_node\"]\n",
    "H_nlp_proj = skill_projection_nlp.subgraph(top_nodes_nlp).copy()\n",
    "pos_proj_nlp = nx.spring_layout(H_nlp_proj, seed=42)\n",
    "nodes = list(H_nlp_proj.nodes())\n",
    "x = [pos_proj_nlp[n][0] for n in nodes]\n",
    "y = [pos_proj_nlp[n][1] for n in nodes]\n",
    "size = [10 + 80 * centrality_nlp.get(n, 0) for n in nodes]\n",
    "\n",
    "fig = go.Figure()\n",
    "for u, v, data in H_nlp_proj.edges(data=True):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[pos_proj_nlp[u][0], pos_proj_nlp[v][0]],\n",
    "            y=[pos_proj_nlp[u][1], pos_proj_nlp[v][1]],\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=max(1, data.get(\"weight\", 1) * 0.2), color=\"#cccccc\"),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode=\"markers+text\",\n",
    "        text=[H_nlp_proj.nodes[n][\"label\"] for n in nodes],\n",
    "        textposition=\"bottom center\",\n",
    "        marker=dict(size=size, color=\"#9467bd\", line=dict(width=1, color=\"#333\")),\n",
    "        hovertext=[\n",
    "            f\"{H_nlp_proj.nodes[n]['label']}<br>Degree: {H_nlp_proj.degree(n)}\"\n",
    "            for n in nodes\n",
    "        ],\n",
    "        hoverinfo=\"text\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"NLP Skill Projection (Top 100 nodes)\",\n",
    "    xaxis=dict(visible=False),\n",
    "    yaxis=dict(visible=False),\n",
    "    margin=dict(l=20, r=20, t=60, b=20),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a106a1",
   "metadata": {},
   "source": [
    "### NLP Skill Islands\n",
    "\n",
    "Apply the same weight-thresholded community detection on the NLP skill projection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_nlp = [data[\"weight\"] for _, _, data in skill_projection_nlp.edges(data=True)]\n",
    "if not weights_nlp:\n",
    "    raise ValueError(\"NLP skill projection has no edges to analyze.\")\n",
    "print(min(weights_nlp), max(weights_nlp))\n",
    "\n",
    "WEIGHT_THRESHOLD_NLP = 20\n",
    "filtered_edges_nlp = [\n",
    "    (u, v)\n",
    "    for u, v, data in skill_projection_nlp.edges(data=True)\n",
    "    if data.get(\"weight\", 0) >= WEIGHT_THRESHOLD_NLP\n",
    "]\n",
    "\n",
    "if not filtered_edges_nlp:\n",
    "    raise ValueError(\n",
    "        \"No NLP edges survived the weight threshold. Adjust WEIGHT_THRESHOLD_NLP or revisit the skill extraction.\"\n",
    "    )\n",
    "\n",
    "island_graph_nlp = skill_projection_nlp.edge_subgraph(filtered_edges_nlp).copy()\n",
    "\n",
    "communities_nlp = list(\n",
    "    nx_comm.greedy_modularity_communities(island_graph_nlp, weight=\"weight\")\n",
    ")\n",
    "community_map_nlp = {\n",
    "    node: idx for idx, nodes in enumerate(communities_nlp, start=1) for node in nodes\n",
    "}\n",
    "\n",
    "community_summary_nlp = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"community_id\": idx,\n",
    "            \"size\": len(nodes),\n",
    "            \"sample_skills\": \", \".join(\n",
    "                sorted(island_graph_nlp.nodes[n][\"label\"] for n in list(nodes)[:5])\n",
    "            ),\n",
    "        }\n",
    "        for idx, nodes in enumerate(communities_nlp, start=1)\n",
    "    ]\n",
    ").sort_values(\"size\", ascending=False)\n",
    "\n",
    "print(f\"Detected {len(communities_nlp)} NLP skill communities after thresholding.\")\n",
    "community_summary_nlp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a73b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = island_graph_nlp\n",
    "pos = nx.spring_layout(H, seed=42, k=0.8)\n",
    "nodes = list(H.nodes())\n",
    "node_x = [pos[n][0] for n in nodes]\n",
    "node_y = [pos[n][1] for n in nodes]\n",
    "node_sizes = [12 + 30 * centrality_nlp.get(n, 0) for n in nodes]\n",
    "communities_cat = pd.Categorical([community_map_nlp.get(n, 0) for n in nodes])\n",
    "\n",
    "fig = go.Figure()\n",
    "for u, v, data in H.edges(data=True):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[pos[u][0], pos[v][0]],\n",
    "            y=[pos[u][1], pos[v][1]],\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=max(1, data.get(\"weight\", 1) * 0.15), color=\"#bbbbbb\"),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode=\"markers+text\",\n",
    "        # text=[H.nodes[n][\"label\"] for n in nodes],\n",
    "        # textposition=\"bottom center\",\n",
    "        marker=dict(\n",
    "            size=node_sizes,\n",
    "            color=communities_cat.codes,\n",
    "            colorscale=\"Plasma\",\n",
    "            line=dict(width=1, color=\"#333\"),\n",
    "        ),\n",
    "        hovertext=[\n",
    "            f\"{H.nodes[n]['label']}<br>Community: {community_map_nlp.get(n)}<br>Degree: {H.degree(n)}\"\n",
    "            for n in nodes\n",
    "        ],\n",
    "        hoverinfo=\"text\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"NLP Skill Islands (communities on weighted projection)\",\n",
    "    xaxis=dict(visible=False),\n",
    "    yaxis=dict(visible=False),\n",
    "    margin=dict(l=20, r=20, t=60, b=20),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_centrality.head(5).to_markdown())\n",
    "print(top_centrality.iloc[20:24].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    top_centrality,\n",
    "    x=\"skill_name\",\n",
    "    y=\"centrality\",\n",
    "    color=\"pipeline\",\n",
    "    barmode=\"group\",\n",
    "    title=\"Top Skill Centrality by Pipeline\",\n",
    "    labels={\"skill_name\": \"Skill\", \"centrality\": \"Degree Centrality\"},\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=45,\n",
    "    xaxis_tickmode=\"linear\",\n",
    "    xaxis_categoryorder=\"total descending\",\n",
    "    yaxis=dict(title=\"Degree Centrality\", rangemode=\"tozero\"),\n",
    "    legend_title=\"Pipeline\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab054917",
   "metadata": {},
   "source": [
    "add## Project Summary\n",
    "\n",
    "We evaluated Alfred job postings through dual job to skill pipelines, building bipartite graphs, Louvain-based skill islands, and centrality comparisons to pinpoint core skill cohorts job seekers should prioritize for targeted search strategies.\n",
    "\n",
    "We can see that the NLP-driven skill extraction uncovers broader skills and denser connections, it is also more prone to noise without careful phrase curation. The dictionary-based approach offers precision but may miss emerging or nuanced skills. Combining both methods could yield a comprehensive skill map for job seekers.\n",
    "\n",
    "The differences in community structures highlight how skill relationships vary based on extraction methods, informing Alfred's job matching algorithms and user guidance. While the job descriptions in this dataset are specific to Alfred's postings, the methodologies applied here can be generalized to other job boards or industries with appropriate adjustments to skill dictionaries and NLP models.\n",
    "Improvement can include implementing a small llm to help rationalize skill extractions and indeed in industry it appears several job boards are implementing some of this functionality already. These include ai powered job description analyzers that suggest skills to add to ones resume based on the job description. But even there, there is significant room for improvement as many of these tools still miss key skills and naunces based of minor textutal differences and human language complexity.\n",
    "\n",
    "The Projects was succeful in demostrating the power of graphs analysis to help job seekers, for instance I realize that focusing on shapening my SQL skills and python along with big data tools like spark and haddop will make my resume appeal more to recruiters in the data engineering field.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
