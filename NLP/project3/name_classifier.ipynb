{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff30b13",
   "metadata": {},
   "source": [
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb5acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math, string\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "import numpy as np, pandas as pd\n",
    "random.seed(6502415)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7215e",
   "metadata": {},
   "source": [
    "lets tale a look at our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4abd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gordon', 'male'), ('Charlott', 'female'), ('Adela', 'female'), ('Daveta', 'female'), ('Torr', 'male'), ('Aube', 'male'), ('Brandise', 'female'), ('Margery', 'female'), ('Darryl', 'male'), ('Neall', 'male')]\n",
      "7944\n",
      "label\n",
      "female    5001\n",
      "male      2943\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "\n",
    "# nltk.download('names') # If needed\n",
    "labeled_names = [(name, \"male\") for name in names.words(\"male.txt\")] + [\n",
    "    (name, \"female\") for name in names.words(\"female.txt\")\n",
    "]\n",
    "random.shuffle(labeled_names)\n",
    "print(labeled_names[:10])\n",
    "print(len(labeled_names))\n",
    "df = pd.DataFrame(labeled_names, columns=[\"name\", \"label\"])\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810640e0",
   "metadata": {},
   "source": [
    "Define features extraction function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ac471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    name = name.lower()\n",
    "\n",
    "    features = {\n",
    "        # General structure\n",
    "        \"first_letter\": name[0],\n",
    "        \"last_letter\": name[-1],\n",
    "        \"last_two\": name[-2:],\n",
    "        \"last_three\": name[-3:],\n",
    "        \"length\": len(name),\n",
    "        \"ends_with_vowel\": name[-1] in \"aeiouy\",# added y to vowel\n",
    "        \n",
    "    }\n",
    "\n",
    "    # --- Domain-informed patterns ---\n",
    "    male_endings = [\"or\", \"son\", \"ric\", \"us\", \"an\", \"ton\", \"bert\", \"ich\",\"on\",\"er\"]\n",
    "    female_endings = [\"a\", \"ia\", \"ie\", \"ine\", \"elle\", \"ette\", \"ina\", \"na\",\"ly\"]\n",
    "\n",
    "    # Flags for specific suffix groups\n",
    "    features[\"male_like\"] = any(name.endswith(suf) for suf in male_endings)\n",
    "    features[\"female_like\"] = any(name.endswith(suf) for suf in female_endings)\n",
    "\n",
    "    \n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69248cb4",
   "metadata": {},
   "source": [
    "Now that we have our labled dataset, we can proceed making test sets, dev-sets and training sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36d0d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = labeled_names[1500:]\n",
    "devtest_names = labeled_names[500:1500]\n",
    "test_names = labeled_names[:500]\n",
    "train_set = [(gender_features2(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features2(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features2(n), gender) for (n, gender) in test_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed8e231",
   "metadata": {},
   "source": [
    "Now we can train our models we are foing a Naive Bayes, a Decision Tree, and a Max Entropy Classifier.\n",
    "\n",
    "Maximum Entropy Implementation: The Maximum Entropy (MaxEnt) model is a probabilistic framework that selects the model with the highest entropy—that ie, the greatest uncertainty or uniformity—subject to the constraints imposed by the training data, ensuring no additional assumptions are made beyond the evidence. The Generalized Iterative Scaling (GIS) algorithm achieves this by updating all feature weights collectively to satisfy global constraints, while the Improved Iterative Scaling (IIS) algorithm refines the process by adjusting each weight individually, resulting in faster convergence and improved numerical stability.\n",
    "\n",
    "Naive Bayes Classifier: The Naive Bayes model is a probabilistic classifier based on Bayes’ Theorem, which predicts the most likely class for a given input by combining prior probabilities with the likelihood of observed features. It assumes that all features are independent of one another (the “naive” assumption), which simplifies computation and often works surprisingly well even when this assumption isn’t perfectly true.\n",
    "\n",
    "Decision Tree Classifier: A Decision Tree classifier predicts outcomes by recursively splitting the data into branches based on feature values that best separate the target classes. Each internal node represents a decision rule, and each leaf node represents a class label; this structure makes the model highly interpretable, though it can sometimes overfit if the tree grows too deep or captures noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adeb74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "dt_classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "maxent_classifier = nltk.MaxentClassifier.train(\n",
    "    train_set,\n",
    "    algorithm=\"IIS\",  # GIS or 'IIS'\n",
    "    trace=0,  # set to 0 to hide iteration output\n",
    "    max_iter=20,  # number of training iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbaad38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04bcbe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acccuracy on dev test-set\n",
      "Decision Tree Accuracy: 0.749\n",
      "Naive Bayes Accuracy: 0.811\n",
      "MaxEnt Accuracy: 0.816\n"
     ]
    }
   ],
   "source": [
    "print(\"Acccuracy on dev test-set\")\n",
    "print(\"Decision Tree Accuracy:\", nltk.classify.accuracy(dt_classifier, devtest_set))\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nltk.classify.accuracy(nb_classifier, devtest_set))\n",
    "\n",
    "print(\"MaxEnt Accuracy:\", nltk.classify.accuracy(maxent_classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7e2578e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set\n",
      "Decision Tree Accuracy: 0.718\n",
      "Naive Bayes Accuracy: 0.788\n",
      "MaxEnt Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy on test set\")\n",
    "print(\"Decision Tree Accuracy:\", nltk.classify.accuracy(dt_classifier, test_set))\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nltk.classify.accuracy(nb_classifier, test_set))\n",
    "\n",
    "print(\"MaxEnt Accuracy:\", nltk.classify.accuracy(maxent_classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a818e",
   "metadata": {},
   "source": [
    "As we can see, the Maximum Entropy model iteratively adjusts feature weights to find the optimal balance for classification. It achieved an accuracy of 0.816 on the training set and 0.788 on the test set, improving upon the baseline accuracy of approximately 0.76 reported in the book. The Naive Bayes achieved an accuracy of .811 on the dev-test set when the letter \"y\" was added to ends_with_vowel feature. The decision tree model was outpreformed by both, all models saw a dip in test_set evaluation. Using the IIS algorithim saw a slight improvement in the test_set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c029900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       |   f     |\n",
      "       |   e     |\n",
      "       |   m   m |\n",
      "       |   a   a |\n",
      "       |   l   l |\n",
      "       |   e   e |\n",
      "-------+---------+\n",
      "female |<270> 37 |\n",
      "  male |  68<125>|\n",
      "-------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_truth = [label for (features, label) in test_set]\n",
    "test_pred = [maxent_classifier.classify(features) for (features, label) in test_set]\n",
    "\n",
    "print(nltk.ConfusionMatrix(test_truth, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1a37b",
   "metadata": {},
   "source": [
    "Taking a look at the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef2e8fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(maxent_classifier.weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f91682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6.462 last_three=='zra' and label is 'male'\n",
      "   4.131 last_three=='eza' and label is 'male'\n",
      "   4.033 last_three=='ido' and label is 'female'\n",
      "   3.780 last_three=='tya' and label is 'male'\n",
      "   3.725 last_three=='tim' and label is 'female'\n",
      "  -3.650 last_letter=='a' and label is 'male'\n",
      "   3.633 last_three=='ild' and label is 'female'\n",
      "   3.469 last_three=='ark' and label is 'female'\n",
      "   3.384 last_three=='bev' and label is 'female'\n",
      "  -3.376 last_two=='ko' and label is 'male'\n",
      "   3.354 last_three=='em' and label is 'female'\n",
      "   3.240 last_three=='pam' and label is 'female'\n",
      "  -3.234 last_letter=='k' and label is 'female'\n",
      "   3.219 last_two=='aa' and label is 'male'\n",
      "   3.219 last_three=='laa' and label is 'male'\n",
      "   3.158 last_two=='ua' and label is 'male'\n",
      "   3.158 last_three=='hua' and label is 'male'\n",
      "  -3.135 last_three=='nne' and label is 'male'\n",
      "   2.999 last_three=='nch' and label is 'female'\n",
      "   2.972 last_three=='kye' and label is 'male'\n"
     ]
    }
   ],
   "source": [
    "maxent_classifier.show_most_informative_features(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cbdbf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                last_two = 'la'           female : male   =     67.0 : 1.0\n",
      "                last_two = 'ia'           female : male   =     48.2 : 1.0\n",
      "             last_letter = 'a'            female : male   =     43.6 : 1.0\n",
      "                last_two = 'ta'           female : male   =     40.9 : 1.0\n",
      "                last_two = 'ra'           female : male   =     33.2 : 1.0\n",
      "                last_two = 'us'             male : female =     32.8 : 1.0\n",
      "                last_two = 'sa'           female : male   =     30.4 : 1.0\n",
      "                last_two = 'rt'             male : female =     27.5 : 1.0\n",
      "             last_letter = 'k'              male : female =     25.3 : 1.0\n",
      "                last_two = 'io'             male : female =     23.7 : 1.0\n",
      "                last_two = 'do'             male : female =     22.6 : 1.0\n",
      "                last_two = 'ld'             male : female =     22.2 : 1.0\n",
      "                last_two = 'rd'             male : female =     21.2 : 1.0\n",
      "              last_three = 'nne'          female : male   =     17.5 : 1.0\n",
      "              last_three = 'ard'            male : female =     17.0 : 1.0\n",
      "              last_three = 'old'            male : female =     16.0 : 1.0\n",
      "              last_three = 'ert'            male : female =     15.1 : 1.0\n",
      "             last_letter = 'f'              male : female =     14.5 : 1.0\n",
      "              last_three = 'ita'          female : male   =     14.4 : 1.0\n",
      "              last_three = 'ela'          female : male   =     13.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
